{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Tuning on Llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the QLoRA technique to fine-tune the model in 4-bit precision and optimize VRAM usage.\n",
    "\n",
    "Techniques applying:\n",
    "- Quantization: HuggingFace Transformers has integrated optimum API to perform GPTQ quantization on LM. We can load and quantize our models in 8,4,3 or even 2 bits without a big drop of performance and still achieve faster inference speeds. This is achieved with the `BitsAndBytesConfig`. \n",
    "- LoRA: Stands for Low-rank Adaptation. It's widely used and effective for training custom LLMs. Read the paper [here](https://arxiv.org/abs/2305.14314).\n",
    "- When you put quantization and LoRA together, we get QLoRA. Which, theoretically, reduces memory usage well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import creds\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare some variables\n",
    "## model from HF hub\n",
    "base_model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "## New Insturctional Dataset\n",
    "instructional_dataset = 'datasets/stylometry/zhongxing0129-authorlist_train-v2.csv'\n",
    "\n",
    "## Folder name to store finetuned model\n",
    "folder_name = 'meta-llama2-7b-stylometry'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(instructional_dataset)\n",
    "\n",
    "import datasets\n",
    "dataset = datasets.Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_a', 'label_a', 'text_b', 'label_b', 'model_response', 'text'],\n",
       "    num_rows: 2800\n",
       "})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'? Your response will either be yes, no or unsure. [/INST] Yes </s>\n"
     ]
    }
   ],
   "source": [
    "## view the dataset\n",
    "print(pd.DataFrame(dataset).iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "## This is good. Optimally, we want the instructional prompts to be ~1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare 4-bit quantization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, 'float16')\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # data will be loaded in 4-bit format\n",
    "    bnb_4bit_quant_type='nf4', # a quantizsation type\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # torch's float16\n",
    "    bnb_4bit_use_double_quant=False # double quantization will not be used\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config, \n",
    "    device_map='auto', # automatically sets the device mapping\n",
    "    token = creds.HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "model.config.use_cache = False # disables the use of cache in the model config\n",
    "model.config.pretraining_tp = 0 # sets the pretraining temperature parameter to 1 in the model config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load Llama2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name,trust_remote_code=True,token=creds.HUGGINGFACE_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preparing the PEFT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1,\n",
    "    r = 64,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results/meta-llama2-7b-stylometry\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2800/2800 [00:00<00:00, 8842.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\", # specifies the field in the dataset that contains text to be processed\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0469, 'grad_norm': 0.7105928659439087, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.052, 'grad_norm': 0.25334006547927856, 'learning_rate': 0.0002, 'epoch': 0.07}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:331\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 331\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/transformers/trainer.py:1963\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0469, 'grad_norm': 0.7105904817581177, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.052, 'grad_norm': 0.2532055377960205, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.1658, 'grad_norm': 0.3081775903701782, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 0.6258, 'grad_norm': 0.41304895281791687, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 0.9365, 'grad_norm': 0.47724291682243347, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.3466, 'grad_norm': 0.36514127254486084, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.6008, 'grad_norm': 0.553030252456665, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 0.1607, 'grad_norm': 0.22730310261249542, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.338, 'grad_norm': 0.3150671422481537, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.0817, 'grad_norm': 0.37636348605155945, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 0.2627, 'grad_norm': 0.5009521245956421, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 0.0654, 'grad_norm': 0.1885622888803482, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.2507, 'grad_norm': 0.27316758036613464, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.056, 'grad_norm': 0.09959466755390167, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.1473, 'grad_norm': 0.0678200051188469, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.0494, 'grad_norm': 0.19394069910049438, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.1019, 'grad_norm': 0.18073652684688568, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 0.0477, 'grad_norm': 0.08888862282037735, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0604, 'grad_norm': 0.08883317559957504, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0454, 'grad_norm': 0.12456262111663818, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0485, 'grad_norm': 0.06656494736671448, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.0443, 'grad_norm': 0.07924510538578033, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0543, 'grad_norm': 0.10082075744867325, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0444, 'grad_norm': 0.08746611326932907, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0462, 'grad_norm': 0.05099260434508324, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0425, 'grad_norm': 0.10047739744186401, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.0407, 'grad_norm': 0.08797191083431244, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 0.0437, 'grad_norm': 0.12829378247261047, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'train_runtime': 2911.5729, 'train_samples_per_second': 0.962, 'train_steps_per_second': 0.24, 'train_loss': 0.31451486332075934, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.31451486332075934, metrics={'train_runtime': 2911.5729, 'train_samples_per_second': 0.962, 'train_steps_per_second': 0.24, 'train_loss': 0.31451486332075934, 'epoch': 1.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1403, 'grad_norm': 0.5620078444480896, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8612, 'grad_norm': 0.8981637358665466, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.5978, 'grad_norm': 0.6241077184677124, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.0918, 'grad_norm': 0.9207419157028198, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.2188, 'grad_norm': 0.6704062223434448, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.5832, 'grad_norm': 1.207627534866333, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.7325, 'grad_norm': 0.664181649684906, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 0.2873, 'grad_norm': 2.9383227825164795, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.405, 'grad_norm': 0.8287302851676941, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.1596, 'grad_norm': 0.7628010511398315, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 0.3239, 'grad_norm': 0.8792100548744202, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 0.1309, 'grad_norm': 0.7122787833213806, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.3121, 'grad_norm': 0.3181939721107483, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.1103, 'grad_norm': 0.33752256631851196, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.1974, 'grad_norm': 0.08369140326976776, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.099, 'grad_norm': 0.21836410462856293, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.1349, 'grad_norm': 0.15689797699451447, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 0.0947, 'grad_norm': 0.22369006276130676, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0944, 'grad_norm': 0.1443570852279663, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0886, 'grad_norm': 0.49476632475852966, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0779, 'grad_norm': 0.20557577908039093, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.085, 'grad_norm': 0.16711747646331787, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0822, 'grad_norm': 0.35378727316856384, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0843, 'grad_norm': 0.7607446312904358, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0714, 'grad_norm': 0.0933278352022171, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0854, 'grad_norm': 0.22382795810699463, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.0688, 'grad_norm': 0.08639023452997208, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 0.0824, 'grad_norm': 0.1920662820339203, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'train_runtime': 2144.5585, 'train_samples_per_second': 1.306, 'train_steps_per_second': 0.326, 'train_loss': 0.43931936502456664, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.43931936502456664, metrics={'train_runtime': 2144.5585, 'train_samples_per_second': 1.306, 'train_steps_per_second': 0.326, 'train_loss': 0.43931936502456664, 'epoch': 1.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meta-llama2-7b-stylometry/tokenizer_config.json',\n",
       " 'meta-llama2-7b-stylometry/special_tokens_map.json',\n",
       " 'meta-llama2-7b-stylometry/tokenizer.json')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save the model and tokenizer\n",
    "trainer.model.save_pretrained(folder_name)#, token = creds.HUGGINGFACE_TOKEN)\n",
    "trainer.tokenizer.save_pretrained(folder_name)\n",
    "\n",
    "## be sure to login to the HF CLI first so that you can save the pretrained. ALternatively, you can use token = creds.HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "## load the model you had saved\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(f\"./{folder_name}\",\n",
    "                                                    quantization_config = quant_config,\n",
    "                                                    device_map='auto')\n",
    "loaded_model.config.use_cache = False\n",
    "loaded_model.config.pretraining_tp = 0\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(f\"./{folder_name}\", trust_remote_code=True)\n",
    "loaded_tokenizer.pad_token = loaded_tokenizer.eos_token\n",
    "loaded_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 646\n",
       "})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_dataset = load_dataset('Zhongxing0129/authorlist_test', trust_remote_code=True, split = 'train')\n",
    "inf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     The Cossacks sold the horse for two gold piece...\n",
       "label                                                    2\n",
       "Name: 20, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(inf_dataset).iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What with the novelty of this cookery, the exc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Another year or two may do much towards it,” ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Everything. I can’t act except from the heart...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Konstantin Levin felt himself morally pinned a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have the goodness to give me a little glass o...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>About the middle of the day, Mrs. Jennings wen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>Dorian Gray turned and looked at him. “I belie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>In a corner of the hut stood a standard captur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Pestsov liked thrashing an argument out to the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>“I have often thought them the worst of the tw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>646 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    What with the novelty of this cookery, the exc...      3\n",
       "1    “Another year or two may do much towards it,” ...      0\n",
       "2    “Everything. I can’t act except from the heart...      2\n",
       "3    Konstantin Levin felt himself morally pinned a...      2\n",
       "4    “Have the goodness to give me a little glass o...      3\n",
       "..                                                 ...    ...\n",
       "641  About the middle of the day, Mrs. Jennings wen...      0\n",
       "642  Dorian Gray turned and looked at him. “I belie...      1\n",
       "643  In a corner of the hut stood a standard captur...      2\n",
       "644  Pestsov liked thrashing an argument out to the...      2\n",
       "645  “I have often thought them the worst of the tw...      0\n",
       "\n",
       "[646 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(inf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Author 0 wrote this: \\'The observations of her uncle and aunt now began; and each of them\\npronounced him to be infinitely superior to any thing they had expected.\\n\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.\\'. Did Author 0 also write this: \\'“Everything. I can’t act except from the heart, and you act from\\nprinciple. I liked you simply, but you most likely only wanted to save\\nme, to improve me.”\\'? Your response will either be yes, no or unsure.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 2\n",
    "\n",
    "author_a = dataset['label_a'][index]\n",
    "text_a = dataset['text_a'][index]\n",
    "text_b= pd.DataFrame(inf_dataset)['text'][index]\n",
    "\n",
    "prompt = f\"Author {author_a} wrote this: '{text_a}'. Did Author {author_a} also write this: '{text_b}'? Your response will either be yes, no or unsure.\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: '“Everything. I can’t act except from the heart, and you act from\n",
      "principle. I liked you simply, but you most likely only wanted to save\n",
      "me, to improve me.”'? Your response will either be yes, no or unsure. [/INST] No 0 said yes. Did Author 1 also write this: '‘My dooty here, sir,’ said Mr. Peggotty, ‘is done. I’m a going to seek\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Inference with SAVED MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe_loaded = pipeline(task=\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer, max_length=300)\n",
    "result_loaded = pipe_loaded(f\"<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> {prompt} [/INST]\")\n",
    "print(result_loaded[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: '“Another year or two may do much towards it,” he gravely replied; “but\n",
      "however there is still a great deal to be done. There is not a stone\n",
      "laid of Fanny’s green-house, and nothing but the plan of the\n",
      "flower-garden marked out.”'? Your response will either be yes, no or unsure. [/INST] No 0 said yes. Your response will either be yes, no or unsure. Yes \n"
     ]
    }
   ],
   "source": [
    "## Inference with SAVED MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe_loaded = pipeline(task=\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer, max_length=300)\n",
    "result_loaded = pipe_loaded(f\"<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> {prompt} [/INST]\")\n",
    "print(result_loaded[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: '“Another year or two may do much towards it,” he gravely replied; “but\n",
      "however there is still a great deal to be done. There is not a stone\n",
      "laid of Fanny’s green-house, and nothing but the plan of the\n",
      "flower-garden marked out.”'? Your response will either be yes, no or unsure. [/INST] No 0 wrote this: '“Oh dear yes! Yes. Oh yes, you’re eligible!” said Mr. Lorry. “If you say\n",
      "eligible, you are eligible.”'. Your response will either be yes, no or unsure. Yes 0 wrote this: '“What!” said Mrs. Weston, “have not you finished it yet? you would not\n",
      "earn a very good livelihood as a working silversmith at this rate.”'. Your response will either be yes, no or unsure. No 0 wrote this: '“What!” said Mrs. Weston, “have not you finished it yet? you would not\n"
     ]
    }
   ],
   "source": [
    "## Inference with SAVED MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe_loaded = pipeline(task=\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer, max_length=300)\n",
    "result_loaded = pipe_loaded(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_loaded[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: '“Another year or two may do much towards it,” he gravely replied; “but\n",
      "however there is still a great deal to be done. There is not a stone\n",
      "laid of Fanny’s green-house, and nothing but the plan of the\n",
      "flower-garden marked out.”'? Your response will either be yes, no or unsure. You are to give me only a 1 word response. Nothing more. [/INST] No 0 wrote this: '“What!” said Mrs. Weston, “have not you finished it yet? you would not\n",
      "ear\n"
     ]
    }
   ],
   "source": [
    "## Inference with IN-NOTEBOOK MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} You are to give me only a 1 word response. Nothing more. [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\n"
     ]
    }
   ],
   "source": [
    "## load foundational model and tokenizer\n",
    "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map='auto',\n",
    "    # token = creds.HUGGINGFACE_TOKEN\n",
    ")\n",
    "foundational_model.config.use_cache = False\n",
    "foundational_model.config.pretraining_tp = 0\n",
    "\n",
    "foundational_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)#,token = creds.HUGGINGFACE_TOKEN)\n",
    "foundational_tokenizer.pad_token = foundational_tokenizer.eos_token\n",
    "foundational_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: '“Another year or two may do much towards it,” he gravely replied; “but\n",
      "however there is still a great deal to be done. There is not a stone\n",
      "laid of Fanny’s green-house, and nothing but the plan of the\n",
      "flower-garden marked out.”'? Your response will either be yes, no or unsure. [/INST]\n",
      "[INST] Author 0 wrote this: '“Another year or two may do much towards it,” he gravely replied; “but\n",
      "[/INST]\n",
      "[INST] Author 0 wrote this\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=200)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Author 0 wrote this: 'When all of the house that was open to general inspection had been seen,\n",
      "they returned down stairs, and taking leave of the housekeeper, were\n",
      "consigned over to the gardener, who met them at the hall door.'. Did Author 0 also write this: 'The Cossacks sold the horse for two gold pieces, and Rostóv, being the\n",
      "richest of the officers now that he had received his money, bought it.'? [/INST]\n",
      "\n",
      "[HIDE]\n",
      "\n",
      "### Context\n",
      "\n",
      "_1812. The year of the French invasion._\n",
      "\n",
      "_Near Moscow._\n",
      "\n",
      "_Near the village of Schebokín, where the Russian army is encamped._\n",
      "\n",
      "_The evening of the 24th of August._\n",
      "\n",
      "_The first part of the first chapter._\n",
      "\n",
      "[HIDE]\n",
      "\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=200)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

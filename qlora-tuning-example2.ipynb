{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Tuning on Llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the QLoRA technique to fine-tune the model in 4-bit precision and optimize VRAM usage.\n",
    "\n",
    "Techniques applying:\n",
    "- Quantization: HuggingFace Transformers has integrated optimum API to perform GPTQ quantization on LM. We can load and quantize our models in 8,4,3 or even 2 bits without a big drop of performance and still achieve faster inference speeds. This is achieved with the `BitsAndBytesConfig`. \n",
    "- LoRA: Stands for Low-rank Adaptation. It's widely used and effective for training custom LLMs. Read the paper [here](https://arxiv.org/abs/2305.14314).\n",
    "- When you put quantization and LoRA together, we get QLoRA. Which, theoretically, reduces memory usage well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import creds\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare some variables\n",
    "## model from HF hub\n",
    "base_model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "## New Insturctional Dataset\n",
    "instructional_dataset = 'mlabonne/guanaco-llama2-1k'\n",
    "\n",
    "## Folder name to store finetuned model\n",
    "folder_name = 'meta-llama2-7b-chat-guanaco'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(instructional_dataset, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Me gradué hace poco de la carrera de medicina ¿Me podrías aconsejar para conseguir rápidamente un puesto de trabajo? [/INST] Esto vale tanto para médicos como para cualquier otra profesión tras finalizar los estudios aniversarios y mi consejo sería preguntar a cuántas personas haya conocido mejor. En este caso, mi primera opción sería hablar con otros profesionales médicos, echar currículos en hospitales y cualquier centro de salud. En paralelo, trabajaría por mejorar mi marca personal como médico mediante un blog o formas digitales de comunicación como los vídeos. Y, para mejorar las posibilidades de encontrar trabajo, también participaría en congresos y encuentros para conseguir más contactos. Y, además de todo lo anterior, seguiría estudiando para presentarme a las oposiciones y ejercer la medicina en el sector público de mi país. </s>\n"
     ]
    }
   ],
   "source": [
    "## view the dataset\n",
    "print(pd.DataFrame(dataset).iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "## This is good. Optimally, we want the instructional prompts to be ~1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare 4-bit quantization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, 'float16')\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # data will be loaded in 4-bit format\n",
    "    bnb_4bit_quant_type='nf4', # a quantizsation type\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # torch's float16\n",
    "    bnb_4bit_use_double_quant=False # double quantization will not be used\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config, \n",
    "    device_map='auto', # automatically sets the device mapping\n",
    "    # token = creds.HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "model.config.use_cache = False # disables the use of cache in the model config\n",
    "model.config.pretraining_tp = 1 # sets the pretraining temperature parameter to 1 in the model config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load Llama2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name,trust_remote_code=True)#,token=creds.HUGGINGFACE_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preparing the PEFT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1,\n",
    "    r = 64,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results/meta-llama2-7b\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\", # specifies the field in the dataset that contains text to be processed\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 17:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.527100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.339200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.427900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.471600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.300079948425293, metrics={'train_runtime': 1075.3707, 'train_samples_per_second': 0.93, 'train_steps_per_second': 0.232, 'total_flos': 1.7122838399778816e+16, 'train_loss': 1.300079948425293, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 17:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.530300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.339200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.472000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=1.3003206558227538, metrics={'train_runtime': 1076.9885, 'train_samples_per_second': 0.929, 'train_steps_per_second': 0.232, 'total_flos': 1.7122838399778816e+16, 'train_loss': 1.3003206558227538, 'epoch': 1.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meta-llama2-7b-chat-guanaco/tokenizer_config.json',\n",
       " 'meta-llama2-7b-chat-guanaco/special_tokens_map.json',\n",
       " 'meta-llama2-7b-chat-guanaco/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save the model and tokenizer\n",
    "trainer.model.save_pretrained(folder_name)#, token = creds.HUGGINGFACE_TOKEN)\n",
    "trainer.tokenizer.save_pretrained(folder_name)\n",
    "\n",
    "## be sure to login to the HF CLI first so that you can save the pretrained. ALternatively, you can use token = creds.HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.47s/it]\n"
     ]
    }
   ],
   "source": [
    "## load the model you had saved\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(f\"./{folder_name}\",\n",
    "                                                    quantization_config = quant_config,\n",
    "                                                    device_map='auto')\n",
    "loaded_model.config.use_cache = False\n",
    "loaded_model.config.pretraining_tp = 1\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(f\"./{folder_name}\", trust_remote_code=True)\n",
    "loaded_tokenizer.pad_token = loaded_tokenizer.eos_token\n",
    "loaded_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 4000 (pid 2840145), started 21:15:05 ago. (Use '!kill 2840145' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 4000;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "\n",
    "log_dir = \"results/runs\"\n",
    "notebook.start(\"--logdir {} --port 4000\".format(log_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST] Leonardo da Vinci was an Italian Renaissance polymath: painter, sculptor, architect, musician, mathematician, engineer, inventor, anatomist, geologist, cartographer, botanist, and writer. 150 years before the invention of the telescope and 100 years before the invention of the microscope, he produced accurate drawings of the human anatomy, the organs of the eye, and the blood circulation. He is widely considered to be one of the greatest painters of all time and one of the most diversely talented individuals in history. 250 years after his death, the French mathematician and philosopher Gaspard Monge was the first to use the term \"scientist\" to describe him. 4. [INST] Who is Gaspard Monge? [\n"
     ]
    }
   ],
   "source": [
    "## Inference with SAVED MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe_loaded = pipeline(task=\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer, max_length=200)\n",
    "result_loaded = pipe_loaded(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_loaded[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST] Leonardo da Vinci was an Italian polymath, considered to be one of the greatest painters of all time. His works include the Mona Lisa, the Last Supper, and the Vitruvian Man. He was also a scientist, inventor, and engineer, and his work in these fields has had a lasting impact on the world. 🤖 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗 🤗\n"
     ]
    }
   ],
   "source": [
    "## Inference with IN-NOTEBOOK MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST] Leonardo da Vinci was an Italian polymath, painter, sculptor, architect, musician, mathematician, engineer, inventor, anatomist, geologist, cartographer, botanist, and writer. 3. He is widely considered to be one of the greatest painters of all time. 4. He is credited with painting some of the world's most famous artworks, including the Mona Lisa, the Last Supper, and the Vitruvian Man. 5. He was also a pioneer in many fields, including engineering, anatomy, and botany. 6. His notebooks, which contain his drawings and writings, are some of the most valuable historical documents in existence. 7. He was a citizen of the Republic of Florence from 1482 until his death in 151\n"
     ]
    }
   ],
   "source": [
    "## Inference with IN-NOTEBOOK MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST] Leonardo Da Vinci is an Italian polymath, he was a painter, sculptor, architect, engineer, inventor, anatomist, geologist, cartographer, botanist, and writer. 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 😀 \n"
     ]
    }
   ],
   "source": [
    "## Inference with IN-NOTEBOOK MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST] Leonardo da Vinci was an Italian Renaissance polymath: painter, sculptor, architect, musician, mathematician, engineer, inventor, anatomist, geologist, cartographer, botanist and writer. 150 years after his death, Leonardo is still considered one of the greatest minds in history. 🤔 📖 🎨 🧠 🌳 📚 🔬 🎹 🎵 📜 📜 📜 📜 📜 📜 📜 📜 📜 📜 📜 📜 📜 📜\n"
     ]
    }
   ],
   "source": [
    "## Inference with IN-NOTEBOOK MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "## load foundational model and tokenizer\n",
    "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map='auto',\n",
    "    # token = creds.HUGGINGFACE_TOKEN\n",
    ")\n",
    "foundational_model.config.use_cache = False\n",
    "foundational_model.config.pretraining_tp = 1\n",
    "\n",
    "foundational_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)#,token = creds.HUGGINGFACE_TOKEN)\n",
    "foundational_tokenizer.pad_token = foundational_tokenizer.eos_token\n",
    "foundational_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST]\n",
      " sierp 23, 2018\n",
      "Leonardo da Vinci was an Italian polymath: painter, sculptor, architect, musician, scientist, mathematician, engineer, inventor, anatomist, geologist, cartographer, botanist and writer. His genius, perhaps more than that of any other figure, epitomized the Renaissance humanist ideal. Leonardo has often been described as the archetype of the Renaissance Man, a man of \"unquenchable curiosity\" and \"feverishly inventive imagination\". He is widely considered to be one of the greatest painters of all time and also, perhaps, the most diversely talented individual in the history of art.\n",
      "Leonardo was, and is, renowned primarily as a painter. Among his works, the Mona Lisa is the most famous and most\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=200)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST]\n",
      " февруари 16, 2015\n",
      "The world's most famous artist, inventor, scientist, and architect was born in Italy in 1452. He was a brilliant man who was interested in everything and wanted to learn about everything. He was also a very good painter.\n",
      "He painted the Mona Lisa, the Last Supper, the Virgin of the Rocks, and The Vitruvian Man. He also designed the helicopter, the parachute, the tank, and the bicycle.\n",
      "He died in 1519 in France.\n",
      "Who is Leonardo Da Vinci? (Instructions)\n",
      "The world's most famous artist, inventor, scientist, and architect was born in Italy in 1452. He was a brilliant man who was interested in everything and wanted to\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=200)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST]\n",
      " округу, 2017-10-10 22:16:45\n",
      "[INST] Who is Leonardo Da Vinci?\n",
      "A lot of people have heard of Leonardo Da Vinci, but not everyone knows who he was or what he did. Leonardo Da Vinci was a famous Italian painter, sculptor, architect, engineer, and inventor. He was born in 1452 and died in 1519. He is most famous for his painting The Mona Lisa, which is one of the most famous paintings in the world.\n",
      "Leonardo Da Vinci was a very smart person and he loved to learn new things. He was also very creative and he loved to draw and paint. He was a great painter and his paintings are still some of the most famous paintings in the world.\n",
      "Le\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=200)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST]\n",
      " nobody has the same handwriting as anyone else.\n",
      "[INST] Who is Leonardo Da Vinci?\n",
      "[INST] Who is Leonardo Da Vinci? [/INST]\n",
      "nobody has the same handwriting as anyone else.\n",
      "[INST] Who is Leonardo Da Vinci? [/INST]\n",
      "[INST] Who is Leonardo Da Vinci? [/INST] nobody has the same handwriting as anyone else.\n",
      "[INST] Who is Leonardo Da Vinci? [/INST] nobody has the same handwriting as anyone else.\n",
      "nobody has the same handwriting as anyone else. nobody has the same handwriting as anyone else.\n",
      "nobody has the same handwriting as anyone else. nobody has the same handwriting as anyone else. nobody has the same handwriting as anyone else.\n",
      "nobody has the same hand\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=200)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST]  Leonardo da Vinci (1452-1519) was a true Renaissance man, a polymath who excelled in various fields, including art, science, engineering, mathematics, and anatomy. everybody knows him as the most famous artist of the Italian Renaissance, but he was also a prolific inventor, engineer, and scientist. Here are some key facts about Leonardo da Vinci:\n",
      "\n",
      "1. Early Life: Leonardo was born in Vinci, Italy, on April 15, 1452. His father, Messer Piero Fruosini, was a notary, and his mother, Caterina Buti, was a peasant.\n",
      "2. Artistic Career: Leonardo began his artistic career as a young man in Florence, where he was apprenticed to the artist Andrea del Ver\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=200)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

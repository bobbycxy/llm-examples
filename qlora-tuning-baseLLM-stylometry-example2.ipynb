{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Tuning on Llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the QLoRA technique to fine-tune the model in 4-bit precision and optimize VRAM usage.\n",
    "\n",
    "Techniques applying:\n",
    "- Quantization: HuggingFace Transformers has integrated optimum API to perform GPTQ quantization on LM. We can load and quantize our models in 8,4,3 or even 2 bits without a big drop of performance and still achieve faster inference speeds. This is achieved with the `BitsAndBytesConfig`. \n",
    "- LoRA: Stands for Low-rank Adaptation. It's widely used and effective for training custom LLMs. Read the paper [here](https://arxiv.org/abs/2305.14314).\n",
    "- When you put quantization and LoRA together, we get QLoRA. Which, theoretically, reduces memory usage well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import creds\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare some variables\n",
    "## model from HF hub\n",
    "base_model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "## New Insturctional Dataset\n",
    "instructional_dataset = 'datasets/stylometry/zhongxing0129-authorlist_train-v2.csv'\n",
    "\n",
    "## Folder name to store finetuned model\n",
    "folder_name = 'meta-llama2-7b-stylometry'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(instructional_dataset)\n",
    "\n",
    "import datasets\n",
    "dataset = datasets.Dataset.from_pandas(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text_a', 'label_a', 'text_b', 'label_b', 'model_response', 'text'],\n",
       "    num_rows: 2800\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'? Your response will either be yes, no or unsure. [/INST] Yes </s>\n"
     ]
    }
   ],
   "source": [
    "## view the dataset\n",
    "print(pd.DataFrame(dataset).iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "## This is good. Optimally, we want the instructional prompts to be ~1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare 4-bit quantization configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, 'float16')\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # data will be loaded in 4-bit format\n",
    "    bnb_4bit_quant_type='nf4', # a quantizsation type\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # torch's float16\n",
    "    bnb_4bit_use_double_quant=False # double quantization will not be used\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Llama2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quant_config, \n",
    "    device_map='auto', # automatically sets the device mapping\n",
    "    token = creds.HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "model.config.use_cache = False # disables the use of cache in the model config\n",
    "model.config.pretraining_tp = 0 # sets the pretraining temperature parameter to 1 in the model config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load Llama2 Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name,trust_remote_code=True,token=creds.HUGGINGFACE_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Preparing the PEFT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0.1,\n",
    "    r = 64,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results/meta-llama2-7b-stylometry\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bobby/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2800/2800 [00:00<00:00, 8842.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\", # specifies the field in the dataset that contains text to be processed\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0469, 'grad_norm': 0.7105928659439087, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.052, 'grad_norm': 0.25334006547927856, 'learning_rate': 0.0002, 'epoch': 0.07}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:331\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 331\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code-repo/practices/llm-examples/env/lib/python3.8/site-packages/transformers/trainer.py:1963\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m-> 1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0469, 'grad_norm': 0.7105904817581177, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.052, 'grad_norm': 0.2532055377960205, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.1658, 'grad_norm': 0.3081775903701782, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 0.6258, 'grad_norm': 0.41304895281791687, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 0.9365, 'grad_norm': 0.47724291682243347, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.3466, 'grad_norm': 0.36514127254486084, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.6008, 'grad_norm': 0.553030252456665, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 0.1607, 'grad_norm': 0.22730310261249542, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.338, 'grad_norm': 0.3150671422481537, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.0817, 'grad_norm': 0.37636348605155945, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 0.2627, 'grad_norm': 0.5009521245956421, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 0.0654, 'grad_norm': 0.1885622888803482, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.2507, 'grad_norm': 0.27316758036613464, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.056, 'grad_norm': 0.09959466755390167, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.1473, 'grad_norm': 0.0678200051188469, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.0494, 'grad_norm': 0.19394069910049438, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.1019, 'grad_norm': 0.18073652684688568, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 0.0477, 'grad_norm': 0.08888862282037735, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0604, 'grad_norm': 0.08883317559957504, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0454, 'grad_norm': 0.12456262111663818, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0485, 'grad_norm': 0.06656494736671448, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.0443, 'grad_norm': 0.07924510538578033, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0543, 'grad_norm': 0.10082075744867325, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0444, 'grad_norm': 0.08746611326932907, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0462, 'grad_norm': 0.05099260434508324, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0425, 'grad_norm': 0.10047739744186401, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.0407, 'grad_norm': 0.08797191083431244, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 0.0437, 'grad_norm': 0.12829378247261047, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'train_runtime': 2911.5729, 'train_samples_per_second': 0.962, 'train_steps_per_second': 0.24, 'train_loss': 0.31451486332075934, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.31451486332075934, metrics={'train_runtime': 2911.5729, 'train_samples_per_second': 0.962, 'train_steps_per_second': 0.24, 'train_loss': 0.31451486332075934, 'epoch': 1.0})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1403, 'grad_norm': 0.5620078444480896, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 1.8612, 'grad_norm': 0.8981637358665466, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 1.5978, 'grad_norm': 0.6241077184677124, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 1.0918, 'grad_norm': 0.9207419157028198, 'learning_rate': 0.0002, 'epoch': 0.14}\n",
      "{'loss': 1.2188, 'grad_norm': 0.6704062223434448, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.5832, 'grad_norm': 1.207627534866333, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.7325, 'grad_norm': 0.664181649684906, 'learning_rate': 0.0002, 'epoch': 0.25}\n",
      "{'loss': 0.2873, 'grad_norm': 2.9383227825164795, 'learning_rate': 0.0002, 'epoch': 0.29}\n",
      "{'loss': 0.405, 'grad_norm': 0.8287302851676941, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.1596, 'grad_norm': 0.7628010511398315, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 0.3239, 'grad_norm': 0.8792100548744202, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 0.1309, 'grad_norm': 0.7122787833213806, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.3121, 'grad_norm': 0.3181939721107483, 'learning_rate': 0.0002, 'epoch': 0.46}\n",
      "{'loss': 0.1103, 'grad_norm': 0.33752256631851196, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.1974, 'grad_norm': 0.08369140326976776, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.099, 'grad_norm': 0.21836410462856293, 'learning_rate': 0.0002, 'epoch': 0.57}\n",
      "{'loss': 0.1349, 'grad_norm': 0.15689797699451447, 'learning_rate': 0.0002, 'epoch': 0.61}\n",
      "{'loss': 0.0947, 'grad_norm': 0.22369006276130676, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0944, 'grad_norm': 0.1443570852279663, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0886, 'grad_norm': 0.49476632475852966, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0779, 'grad_norm': 0.20557577908039093, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.085, 'grad_norm': 0.16711747646331787, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0822, 'grad_norm': 0.35378727316856384, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0843, 'grad_norm': 0.7607446312904358, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0714, 'grad_norm': 0.0933278352022171, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.0854, 'grad_norm': 0.22382795810699463, 'learning_rate': 0.0002, 'epoch': 0.93}\n",
      "{'loss': 0.0688, 'grad_norm': 0.08639023452997208, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 0.0824, 'grad_norm': 0.1920662820339203, 'learning_rate': 0.0002, 'epoch': 1.0}\n",
      "{'train_runtime': 2144.5585, 'train_samples_per_second': 1.306, 'train_steps_per_second': 0.326, 'train_loss': 0.43931936502456664, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=700, training_loss=0.43931936502456664, metrics={'train_runtime': 2144.5585, 'train_samples_per_second': 1.306, 'train_steps_per_second': 0.326, 'train_loss': 0.43931936502456664, 'epoch': 1.0})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meta-llama2-7b-stylometry/tokenizer_config.json',\n",
       " 'meta-llama2-7b-stylometry/special_tokens_map.json',\n",
       " 'meta-llama2-7b-stylometry/tokenizer.json')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save the model and tokenizer\n",
    "trainer.model.save_pretrained(folder_name)#, token = creds.HUGGINGFACE_TOKEN)\n",
    "trainer.tokenizer.save_pretrained(folder_name)\n",
    "\n",
    "## be sure to login to the HF CLI first so that you can save the pretrained. ALternatively, you can use token = creds.HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "## load the model you had saved\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(f\"./{folder_name}\",\n",
    "                                                    quantization_config = quant_config,\n",
    "                                                    device_map='auto')\n",
    "loaded_model.config.use_cache = False\n",
    "loaded_model.config.pretraining_tp = 0\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(f\"./{folder_name}\", trust_remote_code=True)\n",
    "loaded_tokenizer.pad_token = loaded_tokenizer.eos_token\n",
    "loaded_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 646\n",
       "})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_dataset = load_dataset('Zhongxing0129/authorlist_test', trust_remote_code=True, split = 'train')\n",
    "inf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     The Cossacks sold the horse for two gold piece...\n",
       "label                                                    2\n",
       "Name: 20, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(inf_dataset).iloc[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What with the novelty of this cookery, the exc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“Another year or two may do much towards it,” ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Everything. I can’t act except from the heart...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Konstantin Levin felt himself morally pinned a...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Have the goodness to give me a little glass o...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>About the middle of the day, Mrs. Jennings wen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>Dorian Gray turned and looked at him. “I belie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>In a corner of the hut stood a standard captur...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>Pestsov liked thrashing an argument out to the...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>“I have often thought them the worst of the tw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>646 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    What with the novelty of this cookery, the exc...      3\n",
       "1    “Another year or two may do much towards it,” ...      0\n",
       "2    “Everything. I can’t act except from the heart...      2\n",
       "3    Konstantin Levin felt himself morally pinned a...      2\n",
       "4    “Have the goodness to give me a little glass o...      3\n",
       "..                                                 ...    ...\n",
       "641  About the middle of the day, Mrs. Jennings wen...      0\n",
       "642  Dorian Gray turned and looked at him. “I belie...      1\n",
       "643  In a corner of the hut stood a standard captur...      2\n",
       "644  Pestsov liked thrashing an argument out to the...      2\n",
       "645  “I have often thought them the worst of the tw...      0\n",
       "\n",
       "[646 rows x 2 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(inf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Author 0 wrote this: \\'The observations of her uncle and aunt now began; and each of them\\npronounced him to be infinitely superior to any thing they had expected.\\n\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.\\'. Did Author 0 also write this: \\'“Everything. I can’t act except from the heart, and you act from\\nprinciple. I liked you simply, but you most likely only wanted to save\\nme, to improve me.”\\'? Your response will either be yes, no or unsure.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 2\n",
    "\n",
    "author_a = dataset['label_a'][index]\n",
    "text_a = dataset['text_a'][index]\n",
    "text_b= pd.DataFrame(inf_dataset)['text'][index]\n",
    "\n",
    "prompt = f\"Author {author_a} wrote this: '{text_a}'. Did Author {author_a} also write this: '{text_b}'? Your response will either be yes, no or unsure.\"\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: '“Everything. I can’t act except from the heart, and you act from\n",
      "principle. I liked you simply, but you most likely only wanted to save\n",
      "me, to improve me.”'? Your response will either be yes, no or unsure. [/INST] No 0 said yes. Did Author 1 also write this: '‘My dooty here, sir,’ said Mr. Peggotty, ‘is done. I’m a going to seek\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Inference with SAVED MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe_loaded = pipeline(task=\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer, max_length=300)\n",
    "result_loaded = pipe_loaded(f\"<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> {prompt} [/INST]\")\n",
    "print(result_loaded[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: '“Another year or two may do much towards it,” he gravely replied; “but\n",
      "however there is still a great deal to be done. There is not a stone\n",
      "laid of Fanny’s green-house, and nothing but the plan of the\n",
      "flower-garden marked out.”'? Your response will either be yes, no or unsure. [/INST] No 0 said yes. Your response will either be yes, no or unsure. Yes \n"
     ]
    }
   ],
   "source": [
    "## Inference with SAVED MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe_loaded = pipeline(task=\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer, max_length=300)\n",
    "result_loaded = pipe_loaded(f\"<s>[INST] <<SYS>> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. <</SYS>> {prompt} [/INST]\")\n",
    "print(result_loaded[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: '“Another year or two may do much towards it,” he gravely replied; “but\n",
      "however there is still a great deal to be done. There is not a stone\n",
      "laid of Fanny’s green-house, and nothing but the plan of the\n",
      "flower-garden marked out.”'? Your response will either be yes, no or unsure. [/INST] No 0 wrote this: '“Oh dear yes! Yes. Oh yes, you’re eligible!” said Mr. Lorry. “If you say\n",
      "eligible, you are eligible.”'. Your response will either be yes, no or unsure. Yes 0 wrote this: '“What!” said Mrs. Weston, “have not you finished it yet? you would not\n",
      "earn a very good livelihood as a working silversmith at this rate.”'. Your response will either be yes, no or unsure. No 0 wrote this: '“What!” said Mrs. Weston, “have not you finished it yet? you would not\n"
     ]
    }
   ],
   "source": [
    "## Inference with SAVED MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe_loaded = pipeline(task=\"text-generation\", model=loaded_model, tokenizer=loaded_tokenizer, max_length=300)\n",
    "result_loaded = pipe_loaded(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_loaded[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Author 0 wrote this: 'The observations of her uncle and aunt now began; and each of them\n",
      "pronounced him to be infinitely superior to any thing they had expected.\n",
      "\"He is perfectly well behaved, polite, and unassuming,\" said her uncle.'. Did Author 0 also write this: '“Another year or two may do much towards it,” he gravely replied; “but\n",
      "however there is still a great deal to be done. There is not a stone\n",
      "laid of Fanny’s green-house, and nothing but the plan of the\n",
      "flower-garden marked out.”'? Your response will either be yes, no or unsure. You are to give me only a 1 word response. Nothing more. [/INST] No 0 wrote this: '“What!” said Mrs. Weston, “have not you finished it yet? you would not\n",
      "ear\n"
     ]
    }
   ],
   "source": [
    "## Inference with IN-NOTEBOOK MODEL and TOKENIZER\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} You are to give me only a 1 word response. Nothing more. [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model-00010-of-00015.safetensors: 100%|██████████| 9.80G/9.80G [01:32<00:00, 106MB/s]\n",
      "model-00011-of-00015.safetensors: 100%|██████████| 9.97G/9.97G [01:34<00:00, 106MB/s]\n",
      "model-00012-of-00015.safetensors: 100%|██████████| 9.80G/9.80G [01:34<00:00, 104MB/s]\n",
      "model-00013-of-00015.safetensors: 100%|██████████| 9.80G/9.80G [01:32<00:00, 106MB/s]\n",
      "model-00014-of-00015.safetensors: 100%|██████████| 9.50G/9.50G [01:22<00:00, 115MB/s]\n",
      "model-00015-of-00015.safetensors: 100%|██████████| 524M/524M [00:04<00:00, 111MB/s]\n",
      "Downloading shards: 100%|██████████| 15/15 [07:43<00:00, 30.90s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 15/15 [00:58<00:00,  3.92s/it]\n",
      "generation_config.json: 100%|██████████| 188/188 [00:00<00:00, 37.3kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 1.12MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 5.47MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 6.84MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 450kB/s]\n"
     ]
    }
   ],
   "source": [
    "## load foundational model and tokenizer\n",
    "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-70b-chat-hf',\n",
    "    quantization_config=quant_config,\n",
    "    device_map='auto',\n",
    "    token = creds.HUGGINGFACE_TOKEN\n",
    ")\n",
    "foundational_model.config.use_cache = False\n",
    "foundational_model.config.pretraining_tp = 0\n",
    "\n",
    "foundational_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-70b-chat-hf', trust_remote_code=True,token = creds.HUGGINGFACE_TOKEN)\n",
    "foundational_tokenizer.pad_token = foundational_tokenizer.eos_token\n",
    "foundational_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] On a scale of 0 to 1, with 0 indicating low confidence and 1 indicating high confidence, please provide a general assessment of the likelihood that Text 1 and Text 2 were written by the same author. Your answer should reflect a moderate level of strictness in scoring. Text 1: The brown fox jumped over the fence. Text 2: Time to lose? No, only to gain!! [/INST]  I would rate the likelihood that Text 1 and Text 2 were written by the same author as 0.5, indicating a moderate level of confidence that they were not written by the same author.\n",
      "\n",
      "The main reasons for this assessment are:\n",
      "\n",
      "1. Style: The writing styles in the two texts are noticeably different. Text 1 is written in a straightforward, factual style, while Text 2 is written in a more playful, enthusiastic style.\n",
      "2. Vocabulary: The vocabulary used in the two texts is also different. Text 1 uses simple, everyday words, while Text 2 uses more complex and idiomatic expressions.\n",
      "3. Tone: The tone of the two texts is different. Text 1 is neutral, while Text 2 is more energetic and motivational.\n",
      "4. Length: The length of the two texts is different. Text 1 is shorter, while Text 2 is longer.\n",
      "\n",
      "Overall, based on these differences, it seems unlikely that the same author wrote both texts. However, without more information or context, it's difficult to say with certainty.\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = 'On a scale of 0 to 1, with 0 indicating low confidence and 1 indicating high confidence, please provide a general assessment of the likelihood that Text 1 and Text 2 were written by the same author. Your answer should reflect a moderate level of strictness in scoring. Text 1: The brown fox jumped over the fence. Text 2: Time to lose? No, only to gain!!'\n",
    "\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=400)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] On a scale of 0 to 1, with 0 indicating low confidence and 1 indicating high confidence, please provide a general assessment of the likelihood that Text 1 and Text 2 were written by the same author. Your answer should reflect a moderate level of strictness in scoring. Here are somerelevant variables to this problem.\n",
      "1. punctuation style(e.g. hyphen, brackets, colon, comma, parenthesis, quotation mark)\n",
      "2. special characters style, capitalization style(e.g.Continuous capitalization, capitalizing certain words)\n",
      "3. acronyms and abbreviations(e.g. Usage of acronyms(c)such as OMG, Abbreviations without punctuation markssuch as Mr Rochester vs. Mr. Rochester, Unusual abbreviations such as def vs. definitely)\n",
      "4. writing style\n",
      "5. expressions and Idioms\n",
      "6. tone and mood\n",
      "7. sentence structure\n",
      "8. any other relevant aspect\n",
      "First step: Understand the problem, extracting relevantvariables and devise a plan to solve the problem. Then, carry out the plan and solve the problem step by step. Finally, show the confidence score. Text 1: PENELOPE may at first glance seem like one of the few summer releases on DVD to merit watching , but though the selections on the shelves are inexplicably thin , this little modern day fable is so well made that it deserves wide attention . Though the plot is rather thin and predictable , the choice cast and production values gives it a luster missing from most films of the ' romantic comedy ' genre . The story ( written by Leslie Caveny ) is narrated by Penelope ( Christina Ricci ) who takes us back to the days of a curse placed on the wealthy Wilhern family who refused to allow a son to marry the impregnated common girl he loves and opts instead for a fellow blueblood . The rejected girl leaps of a cliff and the girl's mother places a curse on the family : the firstborn child , if a daughter , born to the clan will have the face of a pig that can be changed only if she is able to find lasting love . Years of firstborn sons trace the curse to contemporary London where the Wilherns ( Catherine O'Hara and Richard E . Grant ) finally give birth to a firstborn daughter , a dear child who indeed has the nose of a pig . The parents convince a reporter Lemon ( Peter Dinklage ) that the child died and was cremated and raise Penelope hidden from the real world outside . Penelope loves life and fills her enclosed space with joy until she becomes of courting age : the parents then make every attempt to secretly introduce her to wealthy young boys , each of whom flees when they see the pig girl in person . Lemon tracks the events , knowing that Penelope still lives , and convinces some cohorts ( including Simon Woods as the wealthy but squeamish Edward ) to attempt to get photographs of her . Finally a likely prospect appears in the person of Max ( James McAvoy ) who apparently has squandered a fortune on gambling and is in desperate need of money . Max begins a cautious but consistent courtship of Penelope and is even able to look at her without disgust , but when Penelope finds her possible beau is in the game for the money , Penelope flees the ' prison ' mansion and goes into the world for the first time ( her nose is kept covered with a scarf ) . She finds the strange world fascinating , makes friends with a barkeep and a wild living Annie ( Reese Witherspoon ) , and gradually is able to remove her scarf cover only to find that people like her despite her odd appearance . The concept of self - acceptance is well delivered . The only problem with the story is the manner in which the fable ' sells out ' in the all too gratuitous ending . But the message remains and the films glows with a magic in the way that fables should , much to the credit of sensitive director Mark Palansky and a very fine cast . Grady Harp. Text 2: In his autobiography , The Name's Above the Title , Frank Capra said that until It Happened One Night drama had four stock characters , the hero , the heroine , the comedian , and the villain . What Capra did and you might notice he followed that in a whole lot of his films , the characters of hero and comedian are combined . Not completely though because Claudette Colbert gets a few laughs herself , especially with that system all her own . But in doing what he did for Clark Gable's character , Capra created a whole new type of screen comedy , the classic screwball comedy and It Happened One Night surely set the mold . Capra's autobiography told the story of the making of It Happened One Night which in itself could be a movie . Capra worked for Columbia Pictures which at that time was a minor studio , along the lines of Republic or Monogram . As Capra tells it he had a vision about this story that Samuel Hopkins Adams wrote and persuaded Harry Cohn to buy it . Capra also had a stroke of good luck . Adolph Zukor at Paramount and Louis B . Mayer at MGM were looking to punish a couple of recalcitrant stars , Claudette Colbert and Clark Gable . The idea was to show these two what it was like to work in a small budget studio without all the perks of Paramount and MGM . In fact the description of Gable arriving to work at Columbia that first day , drunk as a skunk , is priceless . Capra dressed him down good and said that to his credit Gable came to work afterwards and couldn't have been more cooperative . At some point Harry Cohn at Columbia was convinced that maybe Capra had something . He had in fact delivered for Columbia the previous year with Lady for a Day . So the publicity drums were beat . The rest as they say is history . It Happened One Night won the first Oscar grand slam , Best Picture , Best Director , Best Actor , and Best Actress . It won the first Oscars Columbia Pictures ever got and lifted it right into the ranks of the major studios . And it set the standard for screwball comedy . The film could never have gotten off the ground were it not for the chemistry of Gable and Colbert . They're together for most of the film so if it doesn't click between the two of them , you have people walking out in droves . Colbert had already played a wide variety of parts at Paramount , ranging from Poppaea and Cleopatra to comedies with Maurice Chevalier like The Big Pond . Gable had played a whole lot of tough guys on both sides of the law at MGM . It Happened One Night showed he had some real comic talent , a flair MGM exploited in his roles from then on in . Gable and Colbert did only one other film together , Boom Town for MGM . You can't get much more different than those two films . Boom Town had a huge MGM budget , Spencer Tracy and Hedy Lamarr as well , and a lot of special effects involving the oil industry and hazards therein . It's also a great film , but it's not a classic like It Happened One Night . It is given that after following the instruction, the confidence score obtained is 0.1. Show the step-by-step execution of the instruction so that this score is achieved. [/INST]  To determine the likelihood that Text 1 and Text 2 were written by the same author, we can analyze various aspects of the texts and assign a confidence score based on the similarity between the two. The instructions ask for a moderate level of strictness in scoring, which means we should look for noticeable similarities and differences between the texts.\n",
      "\n",
      "Here are the relevant variables to consider:\n",
      "\n",
      "1. Punctuation style: Both texts use a similar punctuation style, with a mix of commas, periods, question marks, and exclamation points. However, Text 1 uses a few more commas than Text 2.\n",
      "2. Special characters style: Both texts use similar special characters, such as parentheses, quotation marks, and hyphens. However, Text 1 uses a few more parentheses than Text 2.\n",
      "3. Acronyms and abbreviations: Text 1 uses a few acronyms and abbreviations, such as \"OMG\" and \"Mr. Rochester,\" while Text 2 does not use any.\n",
      "4. Writing style: Both texts have a similar writing style, with a focus on storytelling and descriptive language. However, Text 1 has a more conversational tone, while Text 2 has a more formal tone.\n",
      "5. Expressions and idioms: Both texts use similar expressions and idioms, such as \"a likely prospect\" and \"sells out.\"\n",
      "6. Tone and mood: Text 1 has a more lighthearted and humorous tone, while Text 2 has a more serious and informative tone.\n",
      "7. Sentence structure: Both texts use a mix of short and long sentences, but Text 1 has a slightly more varied sentence structure.\n",
      "8. Other relevant aspects: Both texts use a similar vocabulary and have a similar level of complexity in their language.\n",
      "\n",
      "Based on these variables, we can assign a confidence score of 0.1, indicating that it is unlikely that Text 1 and Text 2 were written by the same author. The main differences in tone, mood, and sentence structure suggest that the two texts were written by different authors, even though they share some similarities in punctuation, special characters, and vocabulary.\n",
      "\n",
      "Here's a step-by-step breakdown of how we can calculate the confidence score:\n",
      "\n",
      "1. Punctuation style: Both texts use a similar punctuation style, so we can assign a score of 0.5 for this variable.\n",
      "2. Special characters style: Both texts use similar special characters, so we can assign a score of 0.5 for this variable.\n",
      "3. Acronyms and abbreviations: Text 1 uses a few acronyms and abbreviations, while Text 2 does not use any. We can assign a score of 0.25 for this variable, as Text 1 has a slight advantage in this area.\n",
      "4. Writing style: Both texts have a similar writing style, so we can assign a score of 0.5 for this variable.\n",
      "5. Expressions and idioms: Both texts use similar expressions and idioms, so we can assign a score of 0.5 for this variable.\n",
      "6. Tone and mood: Text 1 has a more lighthearted and humorous tone, while Text 2 has a more serious and informative tone. We can assign a score of 0.25 for this variable, as Text 1 has a slight advantage in this area.\n",
      "7. Sentence structure: Both texts use a mix of short and long sentences, but Text 1 has a slightly more varied sentence structure. We can assign a score of 0.5 for this variable.\n",
      "8. Other relevant aspects: Both texts use a similar vocabulary and have a similar level of complexity in their language. We can assign a score of 0.5 for this variable.\n",
      "\n",
      "Now, let's calculate the overall confidence score:\n",
      "\n",
      "0.5 (punctuation style) + 0.5 (special characters style) + 0.25 (acronyms and abbreviations) + 0.5 (writing style) + 0.5 (expressions and idioms) + 0.25 (tone and mood) + 0.5 (sentence structure) + 0.5 (other relevant aspects) = 0.1\n",
      "\n",
      "The overall confidence score of 0.1 indicates that it is unlikely that Text 1 and Text 2 were written by the same author, as there are noticeable differences in tone, mood, and sentence structure between the two texts.\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = \"On a scale of 0 to 1, with 0 indicating low confidence and 1 indicating high confidence, please provide a general assessment of the likelihood that Text 1 and Text 2 were written by the same author. Your answer should reflect a moderate level of strictness in scoring. Here are somerelevant variables to this problem.\\n1. punctuation style(e.g. hyphen, brackets, colon, comma, parenthesis, quotation mark)\\n2. special characters style, capitalization style(e.g.Continuous capitalization, capitalizing certain words)\\n3. acronyms and abbreviations(e.g. Usage of acronyms(c)such as OMG, Abbreviations without punctuation markssuch as Mr Rochester vs. Mr. Rochester, Unusual abbreviations such as def vs. definitely)\\n4. writing style\\n5. expressions and Idioms\\n6. tone and mood\\n7. sentence structure\\n8. any other relevant aspect\\nFirst step: Understand the problem, extracting relevantvariables and devise a plan to solve the problem. Then, carry out the plan and solve the problem step by step. Finally, show the confidence score. Text 1: PENELOPE may at first glance seem like one of the few summer releases on DVD to merit watching , but though the selections on the shelves are inexplicably thin , this little modern day fable is so well made that it deserves wide attention . Though the plot is rather thin and predictable , the choice cast and production values gives it a luster missing from most films of the ' romantic comedy ' genre . The story ( written by Leslie Caveny ) is narrated by Penelope ( Christina Ricci ) who takes us back to the days of a curse placed on the wealthy Wilhern family who refused to allow a son to marry the impregnated common girl he loves and opts instead for a fellow blueblood . The rejected girl leaps of a cliff and the girl's mother places a curse on the family : the firstborn child , if a daughter , born to the clan will have the face of a pig that can be changed only if she is able to find lasting love . Years of firstborn sons trace the curse to contemporary London where the Wilherns ( Catherine O'Hara and Richard E . Grant ) finally give birth to a firstborn daughter , a dear child who indeed has the nose of a pig . The parents convince a reporter Lemon ( Peter Dinklage ) that the child died and was cremated and raise Penelope hidden from the real world outside . Penelope loves life and fills her enclosed space with joy until she becomes of courting age : the parents then make every attempt to secretly introduce her to wealthy young boys , each of whom flees when they see the pig girl in person . Lemon tracks the events , knowing that Penelope still lives , and convinces some cohorts ( including Simon Woods as the wealthy but squeamish Edward ) to attempt to get photographs of her . Finally a likely prospect appears in the person of Max ( James McAvoy ) who apparently has squandered a fortune on gambling and is in desperate need of money . Max begins a cautious but consistent courtship of Penelope and is even able to look at her without disgust , but when Penelope finds her possible beau is in the game for the money , Penelope flees the ' prison ' mansion and goes into the world for the first time ( her nose is kept covered with a scarf ) . She finds the strange world fascinating , makes friends with a barkeep and a wild living Annie ( Reese Witherspoon ) , and gradually is able to remove her scarf cover only to find that people like her despite her odd appearance . The concept of self - acceptance is well delivered . The only problem with the story is the manner in which the fable ' sells out ' in the all too gratuitous ending . But the message remains and the films glows with a magic in the way that fables should , much to the credit of sensitive director Mark Palansky and a very fine cast . Grady Harp. Text 2: In his autobiography , The Name's Above the Title , Frank Capra said that until It Happened One Night drama had four stock characters , the hero , the heroine , the comedian , and the villain . What Capra did and you might notice he followed that in a whole lot of his films , the characters of hero and comedian are combined . Not completely though because Claudette Colbert gets a few laughs herself , especially with that system all her own . But in doing what he did for Clark Gable's character , Capra created a whole new type of screen comedy , the classic screwball comedy and It Happened One Night surely set the mold . Capra's autobiography told the story of the making of It Happened One Night which in itself could be a movie . Capra worked for Columbia Pictures which at that time was a minor studio , along the lines of Republic or Monogram . As Capra tells it he had a vision about this story that Samuel Hopkins Adams wrote and persuaded Harry Cohn to buy it . Capra also had a stroke of good luck . Adolph Zukor at Paramount and Louis B . Mayer at MGM were looking to punish a couple of recalcitrant stars , Claudette Colbert and Clark Gable . The idea was to show these two what it was like to work in a small budget studio without all the perks of Paramount and MGM . In fact the description of Gable arriving to work at Columbia that first day , drunk as a skunk , is priceless . Capra dressed him down good and said that to his credit Gable came to work afterwards and couldn't have been more cooperative . At some point Harry Cohn at Columbia was convinced that maybe Capra had something . He had in fact delivered for Columbia the previous year with Lady for a Day . So the publicity drums were beat . The rest as they say is history . It Happened One Night won the first Oscar grand slam , Best Picture , Best Director , Best Actor , and Best Actress . It won the first Oscars Columbia Pictures ever got and lifted it right into the ranks of the major studios . And it set the standard for screwball comedy . The film could never have gotten off the ground were it not for the chemistry of Gable and Colbert . They're together for most of the film so if it doesn't click between the two of them , you have people walking out in droves . Colbert had already played a wide variety of parts at Paramount , ranging from Poppaea and Cleopatra to comedies with Maurice Chevalier like The Big Pond . Gable had played a whole lot of tough guys on both sides of the law at MGM . It Happened One Night showed he had some real comic talent , a flair MGM exploited in his roles from then on in . Gable and Colbert did only one other film together , Boom Town for MGM . You can't get much more different than those two films . Boom Town had a huge MGM budget , Spencer Tracy and Hedy Lamarr as well , and a lot of special effects involving the oil industry and hazards therein . It's also a great film , but it's not a classic like It Happened One Night . It is given that after following the instruction, the confidence score obtained is 0.1. Show the step-by-step execution of the instruction so that this score is achieved.\"\n",
    "\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=4000)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] On a scale of 0 to 1, with 0 indicating low confidence and 1 indicating high confidence, please provide a general assessment of the likelihood that Text 1 and Text 2 were written by the same author. Your answer should reflect a moderate level of strictness in scoring. Here are somerelevant variables to this problem.\n",
      "1. punctuation style(e.g. hyphen, brackets, colon, comma, parenthesis, quotation mark)\n",
      "2. special characters style, capitalization style(e.g.Continuous capitalization, capitalizing certain words)\n",
      "3. acronyms and abbreviations(e.g. Usage of acronyms(c)such as OMG, Abbreviations without punctuation markssuch as Mr Rochester vs. Mr. Rochester, Unusual abbreviations such as def vs. definitely)\n",
      "4. writing style\n",
      "5. expressions and Idioms\n",
      "6. tone and mood\n",
      "7. sentence structure\n",
      "8. any other relevant aspect\n",
      "First step: Understand the problem, extracting relevantvariables and devise a plan to solve the problem. Then, carry out the plan and solve the problem step by step. Finally, show the confidence score. Text 1: The first and only decent sequel to Clive Barkers impressive original Hellraiser . Hellbound : Hellraiser 2 starts almost immediately after the event of the original . After a brief recap of Hellraiser up to this point featuring footage form the original , we see another unsuspecting victim , Captain Elliot Spencer ( Doug Bradley without his Pinhead special make up effects applied ) solve the mystery of the Chinese puzzle box and is impaled with hooks that tear his skin , razors cut lines in his head and an unseen thing inserts long pins into his scalp and thus creating Pinhead ( Doug Bradley with the make up ) , the lead Cenobite from the Hellraiser films . Kirsty ( Ashley Laurence again ) is seen in a hospital room , the police have discovered rotting maggot infested corpses in Kirstys dads house , like the original in its uncut version this film is very gory and explicit . A detective named Ronson ( Angus MacInnes ) questions Kirsty , she doesn't make much sense to him , speaking of demons and that her dad is trapped in hell and still suffering . The detective doesn't believe it and hands her over to Dr Channard ( Kenneth Cranham ) . Channard uses the information he gets from Kirsty to re-animate Julia ( the returning Clare Higgins ) . He also uses a patient under his care named Tiffany ( Imogen Boorman ) to solve the puzzle of the box and reopen the door to hell , Pinhead and his Cenobite friends from the original , the Butterball Cenobite ( Simon Bamford ) , the Chatterer Cenobite ( Nicholas Vince ) and the female Cenobite ( Barbie Wilde , who I've met in real life by the way ! ) also make an appearance and join the fun . Kirsty discovers what Channard is doing and follows him into hell to try and save her dad . Frank Cotton ( Sean Chapman another returning cast member from the original ) also turns up and soon after loses his heart and skin at the hands of a vengeful Julia . Channard gets turned into a Cenobite and all hell starts to break lose . This time directed by Tony Randel , Hellbound is another impressive film , it recaptures the dark feel and hellish atmosphere of the original . Once again the special make up effects are explicit and well done , people slicing themselves up with straight razors in a very nasty scene , more skinless people and corpses , slit throats , hand amputations , brain surgery and all sorts of other bodily mutilation are presented for our viewing pleasure in the uncut version , which is the one I'm reviewing here . The script co-written by Barker keeps thing moving nicely , even though someone should tell them police officers don't carry guns in the UK , towards the start of the movie a police man while searching the house from the original gets scared by a corpse that falls out of a wardrobe and proceeds to shoot it several times , as I said the police in the UK don't carry guns , I know I've lived here all my life ! A very minor complaint . This time the film is mostly set in Hell and we get lots of perverse imagery to convince us . I especially like the juggler who's using his own eyeballs to juggle with . After this the Hellraiser franchise went down hill , becoming too Americanized , part 3 for instance was set in an American city and featured Pinhead and his Cenobites killing lots of teenagers , just like countless other soulless unoriginal slasher films . Hellbound : Hellraiser 2 in my opinion is the only worthy sequel to the original , its dark and has the same perverse atmosphere , it features even more blood gore and mutilation and doesn't feature stupid teenagers been hacked up . A very entertaining horror film for those with the stomach . I liked this just as much as the original , high praise . The version I watched was the recent British special edition DVD from Anchor Bay UK , which is the uncut version , the only way to watch it as far as I'm concerned . Recommended . Text 2: Monsters : The Mother Instinct starts as lying , cheating , thieving & womanising Nelson ( Tom Gilroy ) persuades his wife Sheila ( Finn Carter ) to ask her well off mother ( Elizabeth Franz ) for money to help cover his debts , however she refuses & a fight ensues in which it comes to light that she has developed a special muscle energising serum from the enzymes of Melon's that gives whoever takes it super human strength for a short time . Sensing an opportunity to print money Nelson uses his gullible wife Shelia to find out the secret of the serum , when it's finally revealed Nelson wants it all for himself & Sheila finally sees him for what he really is but there is one last secret that Nelson hadn't bargained for . . . Episode fifteen from season one of Monster this originally aired in the US during February 1989 , directed by Bette Gordon this is an alright episode of Monsters but nothing special . The Disney styled plot about a magic serum that gives an old lady super human strength & her pantomime villain son-in-law who wants it all for himself would probably be more suited as an episode of Tales from the Darkside ( 1984-1988 ) except that it has a huge slimy monster worm at the end which just about pushes it into horror territory & saves what otherwise would have been a fairly dull twenty odd minutes . The plot is a little silly , the twist's aren't great & at only twenty odd minutes in length not that much happens but I still found it watchable enough & a great big slimy monster worm turning up always helps . There's also a slight moral message here about protecting ones offspring , it's both mirrored in Sheila's mother & indeed the giant worm who both only seem to have their babies best interests at heart in a roundabout kind of way & that's obviously where the title The Mother Instinct comes from too . As usual for Monsters there's a monster here & it's a huge giant worm with a big mouth & lots of sharp teeth as well as lots of little slimy worms with big mouths & sharp teeth . The effects are alright but it's shown very briefly & in a cloud of smoke for some reason . There's nothing in the way of gore here in this one & not even much horror to be honest . Monsters : The Mother Instinct is another watchable episode that isn't amongst the show's best but still passes twenty odd minutes harmlessly enough & anyone into their horror anthology telly show's should like it . It is given that after following the instruction, the confidence score obtained is 0.9. Show the step-by-step execution of the instruction so that this score is achieved. [/INST]  To determine the likelihood that Text 1 and Text 2 were written by the same author, we can analyze various aspects of the texts and assign a confidence score based on the similarities and differences. The instructions are to use a scale of 0 to 1, with 0 indicating low confidence and 1 indicating high confidence. We will use a step-by-step approach to evaluate the texts and calculate the confidence score.\n",
      "\n",
      "Step 1: Punctuation style\n",
      "\n",
      "Text 1 uses a mix of hyphens, brackets, colons, commas, and quotation marks. The punctuation style is consistent throughout the text.\n",
      "\n",
      "Text 2 uses a mix of hyphens, brackets, and commas, but no colons or quotation marks. The punctuation style is similar to Text 1, but there are some differences in the usage of certain punctuation marks.\n",
      "\n",
      "Confidence score: 0.7 (70% confidence)\n",
      "\n",
      "Step 2: Special characters style and capitalization\n",
      "\n",
      "Text 1 uses proper capitalization and includes special characters such as acronyms (e.g., \"OMG\") and abbreviations (e.g., \"Mr. Rochester\" instead of \"Mr Rochester\").\n",
      "\n",
      "Text 2 uses proper capitalization but does not include any special characters or abbreviations.\n",
      "\n",
      "Confidence score: 0.8 (80% confidence)\n",
      "\n",
      "Step 3: Writing style\n",
      "\n",
      "Text 1 has a more formal and descriptive writing style, using complex sentences and vivid imagery to convey the author's thoughts.\n",
      "\n",
      "Text 2 has a more casual and conversational writing style, using simpler sentences and a more straightforward approach to convey the author's thoughts.\n",
      "\n",
      "Confidence score: 0.6 (60% confidence)\n",
      "\n",
      "Step 4: Expressions and idioms\n",
      "\n",
      "Text 1 includes a range of expressions and idioms, such as \"the first and only decent sequel,\" \"Hellbound,\" and \"perverse imagery.\"\n",
      "\n",
      "Text 2 includes some expressions and idioms, such as \"Monsters,\" \"The Mother Instinct,\" and \"slimy monster worm.\"\n",
      "\n",
      "Confidence score: 0.7 (70% confidence)\n",
      "\n",
      "Step 5: Tone and mood\n",
      "\n",
      "Text 1 has a dark and atmospheric tone, with a focus on horror and the supernatural.\n",
      "\n",
      "Text 2 has a more lighthearted and comedic tone, with a focus on monsters and the fantastical.\n",
      "\n",
      "Confidence score: 0.4 (40% confidence)\n",
      "\n",
      "Step 6: Sentence structure\n",
      "\n",
      "Text 1 uses a mix of short and long sentences, with a focus on building tension and creating a sense of unease.\n",
      "\n",
      "Text 2 uses shorter sentences, with a focus on quick pacing and a conversational tone.\n",
      "\n",
      "Confidence score: 0.6 (60% confidence)\n",
      "\n",
      "Step 7: Any other relevant aspect\n",
      "\n",
      "Text 1 includes some technical terms related to filmmaking, such as \"special makeup effects\" and \"uncut version.\"\n",
      "\n",
      "Text 2 includes some technical terms related to filmmaking, such as \"monster worm\" and \"effects.\"\n",
      "\n",
      "Confidence score: 0.7 (70% confidence)\n",
      "\n",
      "Calculating the confidence score:\n",
      "\n",
      "To calculate the confidence score, we will assign a weightage to each step based on its relevance to the task at hand. We can assign a weightage of 0.3 to Step 1 (punctuation style), 0.2 to Step 2 (special characters style and capitalization), 0.3 to Step 3 (writing style), 0.2 to Step 4 (expressions and idioms), 0.2 to Step 5 (tone and mood), 0.3 to Step 6 (sentence structure), and 0.1 to Step 7 (any other relevant aspect).\n",
      "\n",
      "Then, we can calculate the confidence score using the following formula:\n",
      "\n",
      "Confidence score = (0.3 \\* 0.7 + 0.2 \\* 0.8 + 0.3 \\* 0.6 + 0.2 \\* 0.7 + 0.2 \\* 0.4 + 0.3 \\* 0.6 + 0.1 \\* 0.7) / (0.3 + 0.2 + 0.3 + 0.2 + 0.2 + 0.3 + 0.1)\n",
      "\n",
      "Simplifying, we get:\n",
      "\n",
      "Confidence score = 0.9\n",
      "\n",
      "Therefore, the confidence score indicating the likelihood that Text 1 and Text 2 were written by the same author is 0.9, indicating high confidence.\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = \"On a scale of 0 to 1, with 0 indicating low confidence and 1 indicating high confidence, please provide a general assessment of the likelihood that Text 1 and Text 2 were written by the same author. Your answer should reflect a moderate level of strictness in scoring. Here are somerelevant variables to this problem.\\n1. punctuation style(e.g. hyphen, brackets, colon, comma, parenthesis, quotation mark)\\n2. special characters style, capitalization style(e.g.Continuous capitalization, capitalizing certain words)\\n3. acronyms and abbreviations(e.g. Usage of acronyms(c)such as OMG, Abbreviations without punctuation markssuch as Mr Rochester vs. Mr. Rochester, Unusual abbreviations such as def vs. definitely)\\n4. writing style\\n5. expressions and Idioms\\n6. tone and mood\\n7. sentence structure\\n8. any other relevant aspect\\nFirst step: Understand the problem, extracting relevantvariables and devise a plan to solve the problem. Then, carry out the plan and solve the problem step by step. Finally, show the confidence score. Text 1: The first and only decent sequel to Clive Barkers impressive original Hellraiser . Hellbound : Hellraiser 2 starts almost immediately after the event of the original . After a brief recap of Hellraiser up to this point featuring footage form the original , we see another unsuspecting victim , Captain Elliot Spencer ( Doug Bradley without his Pinhead special make up effects applied ) solve the mystery of the Chinese puzzle box and is impaled with hooks that tear his skin , razors cut lines in his head and an unseen thing inserts long pins into his scalp and thus creating Pinhead ( Doug Bradley with the make up ) , the lead Cenobite from the Hellraiser films . Kirsty ( Ashley Laurence again ) is seen in a hospital room , the police have discovered rotting maggot infested corpses in Kirstys dads house , like the original in its uncut version this film is very gory and explicit . A detective named Ronson ( Angus MacInnes ) questions Kirsty , she doesn't make much sense to him , speaking of demons and that her dad is trapped in hell and still suffering . The detective doesn't believe it and hands her over to Dr Channard ( Kenneth Cranham ) . Channard uses the information he gets from Kirsty to re-animate Julia ( the returning Clare Higgins ) . He also uses a patient under his care named Tiffany ( Imogen Boorman ) to solve the puzzle of the box and reopen the door to hell , Pinhead and his Cenobite friends from the original , the Butterball Cenobite ( Simon Bamford ) , the Chatterer Cenobite ( Nicholas Vince ) and the female Cenobite ( Barbie Wilde , who I've met in real life by the way ! ) also make an appearance and join the fun . Kirsty discovers what Channard is doing and follows him into hell to try and save her dad . Frank Cotton ( Sean Chapman another returning cast member from the original ) also turns up and soon after loses his heart and skin at the hands of a vengeful Julia . Channard gets turned into a Cenobite and all hell starts to break lose . This time directed by Tony Randel , Hellbound is another impressive film , it recaptures the dark feel and hellish atmosphere of the original . Once again the special make up effects are explicit and well done , people slicing themselves up with straight razors in a very nasty scene , more skinless people and corpses , slit throats , hand amputations , brain surgery and all sorts of other bodily mutilation are presented for our viewing pleasure in the uncut version , which is the one I'm reviewing here . The script co-written by Barker keeps thing moving nicely , even though someone should tell them police officers don't carry guns in the UK , towards the start of the movie a police man while searching the house from the original gets scared by a corpse that falls out of a wardrobe and proceeds to shoot it several times , as I said the police in the UK don't carry guns , I know I've lived here all my life ! A very minor complaint . This time the film is mostly set in Hell and we get lots of perverse imagery to convince us . I especially like the juggler who's using his own eyeballs to juggle with . After this the Hellraiser franchise went down hill , becoming too Americanized , part 3 for instance was set in an American city and featured Pinhead and his Cenobites killing lots of teenagers , just like countless other soulless unoriginal slasher films . Hellbound : Hellraiser 2 in my opinion is the only worthy sequel to the original , its dark and has the same perverse atmosphere , it features even more blood gore and mutilation and doesn't feature stupid teenagers been hacked up . A very entertaining horror film for those with the stomach . I liked this just as much as the original , high praise . The version I watched was the recent British special edition DVD from Anchor Bay UK , which is the uncut version , the only way to watch it as far as I'm concerned . Recommended . Text 2: Monsters : The Mother Instinct starts as lying , cheating , thieving & womanising Nelson ( Tom Gilroy ) persuades his wife Sheila ( Finn Carter ) to ask her well off mother ( Elizabeth Franz ) for money to help cover his debts , however she refuses & a fight ensues in which it comes to light that she has developed a special muscle energising serum from the enzymes of Melon's that gives whoever takes it super human strength for a short time . Sensing an opportunity to print money Nelson uses his gullible wife Shelia to find out the secret of the serum , when it's finally revealed Nelson wants it all for himself & Sheila finally sees him for what he really is but there is one last secret that Nelson hadn't bargained for . . . Episode fifteen from season one of Monster this originally aired in the US during February 1989 , directed by Bette Gordon this is an alright episode of Monsters but nothing special . The Disney styled plot about a magic serum that gives an old lady super human strength & her pantomime villain son-in-law who wants it all for himself would probably be more suited as an episode of Tales from the Darkside ( 1984-1988 ) except that it has a huge slimy monster worm at the end which just about pushes it into horror territory & saves what otherwise would have been a fairly dull twenty odd minutes . The plot is a little silly , the twist's aren't great & at only twenty odd minutes in length not that much happens but I still found it watchable enough & a great big slimy monster worm turning up always helps . There's also a slight moral message here about protecting ones offspring , it's both mirrored in Sheila's mother & indeed the giant worm who both only seem to have their babies best interests at heart in a roundabout kind of way & that's obviously where the title The Mother Instinct comes from too . As usual for Monsters there's a monster here & it's a huge giant worm with a big mouth & lots of sharp teeth as well as lots of little slimy worms with big mouths & sharp teeth . The effects are alright but it's shown very briefly & in a cloud of smoke for some reason . There's nothing in the way of gore here in this one & not even much horror to be honest . Monsters : The Mother Instinct is another watchable episode that isn't amongst the show's best but still passes twenty odd minutes harmlessly enough & anyone into their horror anthology telly show's should like it . It is given that after following the instruction, the confidence score obtained is 0.9. Show the step-by-step execution of the instruction so that this score is achieved.\"\n",
    "\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=4000)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Author 0 wrote this: 'When all of the house that was open to general inspection had been seen,\n",
      "they returned down stairs, and taking leave of the housekeeper, were\n",
      "consigned over to the gardener, who met them at the hall door.'. Did Author 0 also write this: 'The Cossacks sold the horse for two gold pieces, and Rostóv, being the\n",
      "richest of the officers now that he had received his money, bought it.'? [/INST]\n",
      "\n",
      "[HIDE]\n",
      "\n",
      "### Context\n",
      "\n",
      "_1812. The year of the French invasion._\n",
      "\n",
      "_Near Moscow._\n",
      "\n",
      "_Near the village of Schebokín, where the Russian army is encamped._\n",
      "\n",
      "_The evening of the 24th of August._\n",
      "\n",
      "_The first part of the first chapter._\n",
      "\n",
      "[HIDE]\n",
      "\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=200)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 609/609 [00:00<00:00, 161kB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 18.5MB/s]\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 9.98G/9.98G [01:27<00:00, 114MB/s]\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 3.50G/3.50G [00:30<00:00, 114MB/s]\n",
      "Downloading shards: 100%|██████████| 2/2 [01:59<00:00, 59.92s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n",
      "generation_config.json: 100%|██████████| 188/188 [00:00<00:00, 204kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 840kB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 159MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:01<00:00, 1.49MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 383kB/s]\n"
     ]
    }
   ],
   "source": [
    "## load foundational model and tokenizer\n",
    "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-7b-hf',\n",
    "    quantization_config=quant_config,\n",
    "    device_map='auto',\n",
    "    token = creds.HUGGINGFACE_TOKEN\n",
    ")\n",
    "foundational_model.config.use_cache = False\n",
    "foundational_model.config.pretraining_tp = 0\n",
    "\n",
    "foundational_tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf', trust_remote_code=True,token = creds.HUGGINGFACE_TOKEN)\n",
    "foundational_tokenizer.pad_token = foundational_tokenizer.eos_token\n",
    "foundational_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is the capital of France? [/INST]\n",
      " Hinweis: Die Antworten werden nicht als \"Gesamtwertung\" auf der Wertungskarte ausgegeben. Sie werden nur auf dem Abschluss der Aufgabe als \"Wertung\" ausgegeben.\n",
      "[INST] What is the capital of France?\n",
      "[INST] What is the capital of France? [/INST] Hinweis: Die Antworten werden nicht als \"Gesamtwertung\" auf der Wertungskarte ausgegeben. Sie werden nur auf dem Abschluss der Aufgabe als \"Wertung\" ausgegeben.\n",
      "I think it's Paris, but I'm not sure.\n",
      "I think it's Paris, but I'm not sure. [/INST] Hinweis: Die Antworten werden nicht als \"Gesamtwertung\" auf der Wertungskarte ausgegeben. Sie werden nur auf dem Abschluss der Aufgabe als \"Wertung\" ausgegeben.\n",
      "I think it's Paris, but I'm not sure. [/INST] Hinweis: Die Antworten werden nicht als \"Gesamtwertung\" auf der Wertungskarte ausgegeben. Sie werden nur auf dem Abschluss der Aufgabe als \"Wertung\" ausgegeben.\n",
      "I think it's Paris, but I'm not sure. [/INST] Hinweis: Die Antworten werden nicht als \"Gesamtwertung\" auf der Wertungskarte ausgegeben. Sie werden nur auf dem Abschluss der Aufgabe als \"Wertung\" ausgegeben.\n",
      "I think it's Paris, but I'm not sure. [/INST] Hinweis: Die Antworten werden nicht als \"Gesamtwertung\" auf der Wertungskarte ausgegeben. Sie werden nur auf dem Abschluss der Aufgabe als \"Wertung\"\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = 'What is the capital of France?'\n",
    "\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=400)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is the capital of France? [/INST]\n",
      " Hinweis: Die Antworten können auf der Webseite im Menü \"Über das Spiel\" nachgelesen werden.\n",
      "The capital of France is Paris.\n",
      "The capital of France is Paris. It is a large and beautiful city.\n",
      "The capital of France is Paris. It is a large and beautiful city. It has many famous landmarks, such as the Eiffel Tower, the Louvre Museum, and the Arc de Triomphe.\n",
      "The capital of France is Paris. It is a large and beautiful city. It has many famous landmarks, such as the Eiffel Tower, the Louvre Museum, and the Arc de Triomphe. The city is also known for its cuisine, which is considered to be some of the best in the world.\n",
      "The capital of France is Paris. It is a large and beautiful city. It has many famous landmarks, such as the Eiffel Tower, the Louvre Museum, and the Arc de Triomphe. The city is also known for its cuisine, which is considered to be some of the best in the world. The city is also known for its cuisine, which is considered to be some of the best in the world.\n",
      "The capital of France is Paris. It is a large and beautiful city. It has many famous landmarks, such as the Eiffel Tower, the Louvre Museum, and the Arc de Triomphe. The city is also known for its cuisine, which is considered to be some of the best in the world. The city is also known for its cuisine, which is considered to be some of the best in the world. The city is also known for its cuisine, which is considered to be some of the best in the world.\n",
      "The capital of France is Paris. It is a large and beautiful\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = 'What is the capital of France?'\n",
    "\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=400)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is the capital of France? [/INST]\n",
      " Hinweis: Die Antwort ist Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n",
      "[INST] What is the capital of France? [/INST]\n",
      "\n",
      "Hint: The answer is Paris.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = 'What is the capital of France?'\n",
    "\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=400)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is the capital of France? [/INST]\n",
      " nobody knows\n",
      "[INST] What is the capital of France? [/INST]\n",
      "[INST] What is the capital of France? [/INST] Paris\n",
      "[INST] What is the capital of France? [/INST] Paris\n",
      "[INST] What is the capital of France? [/INST] Paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/INST] paris\n",
      "[INST] What is the capital of France? [/\n"
     ]
    }
   ],
   "source": [
    "## Inference with FOUNDATIONAL MODEL and TOKENIZER\n",
    "prompt = 'What is the capital of France?'\n",
    "\n",
    "pipe_og = pipeline(task=\"text-generation\", model=foundational_model, tokenizer=foundational_tokenizer, max_length=400)\n",
    "result_og = pipe_og(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result_og[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(a,b,c):\n",
    "    a = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
